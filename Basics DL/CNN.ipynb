{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c1ba425",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ff3059",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network ( CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb0e838",
   "metadata": {},
   "source": [
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fb3c7d",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691f5f9b",
   "metadata": {},
   "source": [
    "#### Step 1 - Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "957b6a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b164083",
   "metadata": {},
   "source": [
    "#### Step 2 - Preprocessing the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ea8c3e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# this is the augmentation configuration we will use for training\n",
    "\n",
    "train_datagen = ImageDataGenerator( rescale = 1./255,\n",
    "                                    shear_range = 0.2,\n",
    "                                    zoom_range = 0.2,\n",
    "                                    horizontal_flip = True)\n",
    "\n",
    "# this is a generator that will read pictures found in subfolers of 'path_below'\n",
    "# and indefinitely generate batches of augmented image data\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "    \"C:/Users/tze/OneDrive/ML_BOOTCAMP/DEEP LEARNING/dataset/training_set\",  # this is the target directory\n",
    "    target_size= (64,64),                                                    # all images will be resized to 64x64\n",
    "    batch_size= 32,\n",
    "    class_mode='binary')                                                     # we are using a binary classification mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d080fc5",
   "metadata": {},
   "source": [
    "#### Step 3 - Preprocessing the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bce41e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# We only need the rescaling for the augmentation configuration of \n",
    "# the test set because we don't want to change them as it behaves \n",
    "# like new images \n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "    \"C:/Users/tze/OneDrive/ML_BOOTCAMP/DEEP LEARNING/dataset/test_set\",  \n",
    "    target_size= (64,64),                                                   \n",
    "    batch_size= 32,\n",
    "    class_mode='binary') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736f3bd9",
   "metadata": {},
   "source": [
    "### Building CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae13e3e5",
   "metadata": {},
   "source": [
    "#### Initializing the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1d57fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0448d8dc",
   "metadata": {},
   "source": [
    "#### Step 1 - Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9980ff1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters = 32, kernel_size = 3, activation = 'relu', input_shape = [64, 64, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae04155c",
   "metadata": {},
   "source": [
    "#### Step 2 - Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3be57634",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size = 2, strides = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149dad08",
   "metadata": {},
   "source": [
    "#### Step 3 - Second Convolutional and Pooling layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3201fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters = 32, kernel_size = 3, activation = 'relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size = 2, strides = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8561cb4c",
   "metadata": {},
   "source": [
    "#### Step 4 - Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0557e681",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50ec03d",
   "metadata": {},
   "source": [
    "#### Step 5 - Full Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "297c668a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units = 128, activation = 'relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2379d990",
   "metadata": {},
   "source": [
    "#### Step 6 - Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c29bd0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units = 1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcab5aa2",
   "metadata": {},
   "source": [
    "### Training the CNN "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d007fe",
   "metadata": {},
   "source": [
    "#### Step 1 - Compiling the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c5e8146",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad22840",
   "metadata": {},
   "source": [
    "#### Step 2 - Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89b524a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - 221s 877ms/step - loss: 0.6735 - accuracy: 0.5673 - val_loss: 0.6464 - val_accuracy: 0.6080\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 83s 333ms/step - loss: 0.6158 - accuracy: 0.6562 - val_loss: 0.5960 - val_accuracy: 0.6840\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 78s 310ms/step - loss: 0.5701 - accuracy: 0.6988 - val_loss: 0.5552 - val_accuracy: 0.7215\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 63s 252ms/step - loss: 0.5425 - accuracy: 0.7231 - val_loss: 0.5622 - val_accuracy: 0.7230\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 128s 510ms/step - loss: 0.5250 - accuracy: 0.7311 - val_loss: 0.5063 - val_accuracy: 0.7600\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 76s 303ms/step - loss: 0.4940 - accuracy: 0.7601 - val_loss: 0.5056 - val_accuracy: 0.7560\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 78s 314ms/step - loss: 0.4820 - accuracy: 0.7661 - val_loss: 0.4792 - val_accuracy: 0.7665\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 76s 306ms/step - loss: 0.4641 - accuracy: 0.7814 - val_loss: 0.4609 - val_accuracy: 0.7865\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 77s 309ms/step - loss: 0.4509 - accuracy: 0.7829 - val_loss: 0.4766 - val_accuracy: 0.7845\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 78s 310ms/step - loss: 0.4496 - accuracy: 0.7880 - val_loss: 0.4650 - val_accuracy: 0.7890\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 72s 287ms/step - loss: 0.4259 - accuracy: 0.8016 - val_loss: 0.4630 - val_accuracy: 0.7885\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 74s 294ms/step - loss: 0.4124 - accuracy: 0.8121 - val_loss: 0.4742 - val_accuracy: 0.7910\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 71s 286ms/step - loss: 0.3904 - accuracy: 0.8275 - val_loss: 0.4616 - val_accuracy: 0.8030\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 70s 281ms/step - loss: 0.3750 - accuracy: 0.8311 - val_loss: 0.4464 - val_accuracy: 0.8065\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 68s 271ms/step - loss: 0.3644 - accuracy: 0.8350 - val_loss: 0.4729 - val_accuracy: 0.8025\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 63s 252ms/step - loss: 0.3439 - accuracy: 0.8481 - val_loss: 0.4727 - val_accuracy: 0.8010\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 66s 262ms/step - loss: 0.3428 - accuracy: 0.8490 - val_loss: 0.4907 - val_accuracy: 0.7925\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 57s 228ms/step - loss: 0.3192 - accuracy: 0.8627 - val_loss: 0.4913 - val_accuracy: 0.7975\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 58s 231ms/step - loss: 0.3082 - accuracy: 0.8723 - val_loss: 0.4874 - val_accuracy: 0.7930\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 0.2983 - accuracy: 0.8709 - val_loss: 0.4663 - val_accuracy: 0.8065\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 57s 229ms/step - loss: 0.2859 - accuracy: 0.8760 - val_loss: 0.4699 - val_accuracy: 0.8020\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 0.2714 - accuracy: 0.8850 - val_loss: 0.4985 - val_accuracy: 0.8110\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 57s 230ms/step - loss: 0.2574 - accuracy: 0.8891 - val_loss: 0.4907 - val_accuracy: 0.8120\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 1312s 5s/step - loss: 0.2411 - accuracy: 0.9036 - val_loss: 0.5183 - val_accuracy: 0.8060\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 47s 187ms/step - loss: 0.2376 - accuracy: 0.9035 - val_loss: 0.5414 - val_accuracy: 0.7900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b922feff10>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x = training_set, validation_data = test_set, epochs = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc4d85b",
   "metadata": {},
   "source": [
    "### Making a single value prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc080a92",
   "metadata": {},
   "source": [
    "#### Prediction of random image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d98da84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 225ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "test_image = tf.keras.preprocessing.image.load_img('C:/Users/tze/OneDrive/ML_BOOTCAMP/DEEP LEARNING/dataset/télécharger.jpg', grayscale = True, target_size = (64,64))\n",
    "test_image = tf.keras.preprocessing.image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "prediction = cnn.predict(test_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80af351",
   "metadata": {},
   "source": [
    "#### Output configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18e8f98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dog\n"
     ]
    }
   ],
   "source": [
    "training_set.class_indices\n",
    "if prediction[0][0] == 1:\n",
    "    output = 'Dog'\n",
    "else:\n",
    "    output = 'Cat'\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e6ba1f",
   "metadata": {},
   "source": [
    "#### Predicting Single value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
